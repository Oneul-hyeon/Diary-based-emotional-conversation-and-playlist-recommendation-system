{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/toy-f-rebellion/toy_ai.git\n",
        "%cd /content/toy_ai\n",
        "!git clone https://github.com/Oneul-hyeon/Modified-KoBERT.git\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "358zkRZEdUvC",
        "outputId": "f23a3eb0-3231-476b-cda4-3123127b0122"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'toy_ai'...\n",
            "remote: Enumerating objects: 65, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 65 (delta 15), reused 14 (delta 3), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (65/65), 1.57 MiB | 4.77 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n",
            "/content/toy_ai\n",
            "Cloning into 'Modified-KoBERT'...\n",
            "remote: Enumerating objects: 449, done.\u001b[K\n",
            "remote: Counting objects: 100% (174/174), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 449 (delta 136), reused 118 (delta 108), pack-reused 275\u001b[K\n",
            "Receiving objects: 100% (449/449), 226.32 KiB | 5.14 MiB/s, done.\n",
            "Resolving deltas: 100% (227/227), done.\n",
            "Collecting kobert_tokenizer (from -r requirements.txt (line 11))\n",
            "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-1vltc20n/kobert-tokenizer_efbb4502afb94f34b48b254aa812fc3f\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-1vltc20n/kobert-tokenizer_efbb4502afb94f34b48b254aa812fc3f\n",
            "  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boto3<=1.15.18 (from -r requirements.txt (line 1))\n",
            "  Downloading boto3-1.15.18-py2.py3-none-any.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gluonnlp==0.8.0 (from -r requirements.txt (line 2))\n",
            "  Downloading gluonnlp-0.8.0.tar.gz (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mxnet (from -r requirements.txt (line 3))\n",
            "  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime==1.12.0 (from -r requirements.txt (line 4))\n",
            "  Downloading onnxruntime-1.12.0-cp310-cp310-manylinux_2_27_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece==0.1.96 (from -r requirements.txt (line 5))\n",
            "  Downloading sentencepiece-0.1.96-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (2.0.1+cu118)\n",
            "Collecting transformers (from -r requirements.txt (line 7))\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.66.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gluonnlp==0.8.0->-r requirements.txt (line 2)) (1.23.5)\n",
            "Collecting coloredlogs (from onnxruntime==1.12.0->-r requirements.txt (line 4))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.12.0->-r requirements.txt (line 4)) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.12.0->-r requirements.txt (line 4)) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.12.0->-r requirements.txt (line 4)) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime==1.12.0->-r requirements.txt (line 4)) (1.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (4.7.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 6)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 6)) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 6)) (16.0.6)\n",
            "Collecting botocore<1.19.0,>=1.18.18 (from boto3<=1.15.18->-r requirements.txt (line 1))\n",
            "  Downloading botocore-1.18.18-py2.py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3<=1.15.18->-r requirements.txt (line 1))\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3<=1.15.18->-r requirements.txt (line 1))\n",
            "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet->-r requirements.txt (line 3)) (2.31.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet->-r requirements.txt (line 3))\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers->-r requirements.txt (line 7))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 7)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 7)) (2023.6.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 7))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 7))\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 9)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 9)) (2023.3)\n",
            "Collecting urllib3<1.26,>=1.20 (from botocore<1.19.0,>=1.18.18->boto3<=1.15.18->-r requirements.txt (line 1))\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers->-r requirements.txt (line 7)) (2023.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 9)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet->-r requirements.txt (line 3)) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet->-r requirements.txt (line 3)) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet->-r requirements.txt (line 3)) (2023.7.22)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime==1.12.0->-r requirements.txt (line 4))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 6)) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime==1.12.0->-r requirements.txt (line 4)) (1.3.0)\n",
            "Building wheels for collected packages: gluonnlp, kobert_tokenizer\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gluonnlp: filename=gluonnlp-0.8.0-py3-none-any.whl size=292696 sha256=bb9edf9d87cc063b872465dc7c8aeda3332013b2e0a9506950c9c2e94a809e95\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/cc/dc/7ec84dced25f738b8be400101abb67e4b50c905090a51017e4\n",
            "  Building wheel for kobert_tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kobert_tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4633 sha256=138e7d0a455122c61cd484c6c4cec7019751b756f1ce7038c749230f7007b470\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5r1vcg44/wheels/e9/1a/3f/a864970e8a169c176befa3c4a1e07aa612f69195907a4045fe\n",
            "Successfully built gluonnlp kobert_tokenizer\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, kobert_tokenizer, urllib3, jmespath, humanfriendly, graphviz, gluonnlp, coloredlogs, botocore, s3transfer, onnxruntime, mxnet, huggingface-hub, transformers, boto3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.4\n",
            "    Uninstalling urllib3-2.0.4:\n",
            "      Successfully uninstalled urllib3-2.0.4\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.20.1\n",
            "    Uninstalling graphviz-0.20.1:\n",
            "      Successfully uninstalled graphviz-0.20.1\n",
            "Successfully installed boto3-1.15.18 botocore-1.18.18 coloredlogs-15.0.1 gluonnlp-0.8.0 graphviz-0.8.4 huggingface-hub-0.16.4 humanfriendly-10.0 jmespath-0.10.0 kobert_tokenizer-0.1 mxnet-1.9.1 onnxruntime-1.12.0 s3transfer-0.3.7 safetensors-0.3.3 sentencepiece-0.1.96 tokenizers-0.13.3 transformers-4.32.0 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNjzjyZLVU07"
      },
      "source": [
        "# 1. Environment Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mWE5F3FVU08"
      },
      "source": [
        "## (1) 라이브러리 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9n1-B_V8VU09"
      },
      "outputs": [],
      "source": [
        "# Setting Library\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import sys\n",
        "sys.path.append('/content/toy_ai/Modified-KoBERT')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gluonnlp as nlp\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pickle\n",
        "\n",
        "# koBERT\n",
        "from kobert.utils import get_tokenizer\n",
        "from kobert.pytorch_kobert import get_pytorch_kobert_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQW0ZxLdVU0-"
      },
      "source": [
        "## (2). 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jCKFbl08VU0-"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/toy_ai/Dataset/5차년도_2차.csv', encoding = 'CP949')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (3) KoBERT 모델 불러오기"
      ],
      "metadata": {
        "id": "o-VhYTtNdSyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bertmodel, vocab = get_pytorch_kobert_model(cachedir=\".cache\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbTPLXXcdZ7A",
        "outputId": "8281a5f2-7310-4f11-ec7c-520752abfdaf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/toy_ai/.cache/kobert_v1.zip[██████████████████████████████████████████████████]\n",
            "/content/toy_ai/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece[██████████████████████████████████████████████████]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXNbwJZ1VU0-"
      },
      "source": [
        "# 2. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT2D8mQsVU0-"
      },
      "source": [
        "## (1) Labeling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 라벨값 확인"
      ],
      "metadata": {
        "id": "yVYETVzDbec2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UKcIqT3QVU0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d53fbcff-5cf5-47ed-c75c-1823c439e885"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['happiness', 'neutral', 'sadness', 'angry', 'surprise', 'disgust',\n",
              "       'fear'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "data['상황'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 라벨링"
      ],
      "metadata": {
        "id": "1TMRPLNUbheJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = list(data['상황'].unique())\n",
        "for i in range(len(labels)) :\n",
        "  data.loc[data['상황'] == labels[i], '상황'] = i"
      ],
      "metadata": {
        "id": "3v2F5cgJbjb7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mbXi44RVU0_"
      },
      "source": [
        "## (2) KoBERT 모델의 입력 데이터 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RyFeOJhtVU0_"
      },
      "outputs": [],
      "source": [
        "dataset = []\n",
        "for text, label in zip(data['발화문'], data['상황'])  :\n",
        "    data = []\n",
        "    data.append(text)\n",
        "    data.append(str(label))\n",
        "    dataset.append(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5jGWiPfVU0_"
      },
      "source": [
        "## (3) Train/Test Dataset Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sy7vKhhzVU0_"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = train_test_split(dataset, test_size = 0.2,random_state = 42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (4) 토큰화"
      ],
      "metadata": {
        "id": "M_6irrlNb_Sz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 함수 정의"
      ],
      "metadata": {
        "id": "5FbtcJnhcBgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 데이터가 BERT 모델의 입력으로 들어갈 수 있도록 함수 정의\n",
        "class BERTDataset(Dataset):\n",
        "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
        "                 pad, pair):\n",
        "\n",
        "        transform = nlp.data.BERTSentenceTransform(\n",
        "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
        "\n",
        "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
        "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
        "    def __getitem__(self, i):\n",
        "        return (self.sentences[i] + (self.labels[i], ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.labels))"
      ],
      "metadata": {
        "id": "Ju42Dy02cAz4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer()\n",
        "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhmNuhmXcGO7",
        "outputId": "f7852cee-3d69-4f7b-edc1-8f3cbef2c488"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using cached model. /content/toy_ai/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Setting parameters"
      ],
      "metadata": {
        "id": "al2z6bCocI80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 64\n",
        "batch_size = 64\n",
        "warmup_ratio = 0.1\n",
        "num_epochs = 1000\n",
        "max_grad_norm = 1\n",
        "log_interval = 200\n",
        "learning_rate =  5e-5"
      ],
      "metadata": {
        "id": "2TFV6E73cGsC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 토큰화"
      ],
      "metadata": {
        "id": "lF51xLeld_kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = BERTDataset(train_data, 0, 1, tok, max_len, True, False)\n",
        "test_data = BERTDataset(test_data,0, 1, tok, max_len, True, False)"
      ],
      "metadata": {
        "id": "iUMBy6YbcIIp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- torch 형식으로 반환"
      ],
      "metadata": {
        "id": "hVM9OOvoeHyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=5)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=5)"
      ],
      "metadata": {
        "id": "Y70Y5BJZeI5R"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 데이터 저장"
      ],
      "metadata": {
        "id": "rYtuvlsleT9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataloder를 파일에 저장\n",
        "with open('/content/toy_ai/train_dataloder.pkl', 'wb') as f:\n",
        "    pickle.dump(train_dataloader, f)\n",
        "# test_dataloder를 파일에 저장\n",
        "with open('/content/toy_ai/test_dataloader.pkl', 'wb') as f:\n",
        "    pickle.dump(test_dataloader, f)"
      ],
      "metadata": {
        "id": "xrqOd57GeKD4"
      },
      "execution_count": 14,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}